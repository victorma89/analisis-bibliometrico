import os
import itertools
import networkx as nx
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import io, base64
import bibtexparser
import importlib
from collections import Counter
import re

# --- Importar Requerimientos 3 y 4 ---
analizador_frecuencias = importlib.import_module("app.3_frecuencia_palabras.analizador_frecuencias")
analizador_cluster = importlib.import_module("app.4_agrupamiento_jerarquico.analizador_cluster")

# === Ruta de datos ===
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
BIB_PATH = os.path.join(ROOT_DIR, 'datos', 'procesados', 'articulos_unicos.bib')

# === Funci√≥n auxiliar: cargar abstracts ===
def _cargar_abstracts():
    if not os.path.exists(BIB_PATH):
        raise FileNotFoundError(f"No se encontr√≥ el archivo: {BIB_PATH}")
    with open(BIB_PATH, 'r', encoding='utf-8') as f:
        parser = bibtexparser.bparser.BibTexParser(common_strings=False)
        parser.ignore_errors = True
        db = bibtexparser.load(f, parser=parser)
    return [entry['abstract'] for entry in db.entries if 'abstract' in entry]

# === Construcci√≥n del grafo de coocurrencia ===
def construir_grafo_coocurrencia(abstracts, terminos):
    """
    Construye un grafo de coocurrencia usando b√∫squeda de palabras completas.
    """
    G = nx.Graph()
    
    print(f"\nüîç Construyendo grafo con {len(terminos)} t√©rminos...")
    print(f"   T√©rminos: {terminos[:10]}...")  # Mostrar primeros 10
    
    # üìä Contador de apariciones por t√©rmino
    contador_terminos = Counter()
    
    # üìä Contador de coocurrencias
    coocurrencias_debug = []
    
    for idx, abstract in enumerate(abstracts):
        if idx % 100 == 0:
            print(f"   Procesando abstract {idx}/{len(abstracts)}...")
        
        texto = abstract.lower()
        
        # ‚úÖ B√∫squeda de palabras completas usando regex
        presentes = []
        for termino in terminos:
            pattern = r'\b' + re.escape(termino.lower()) + r'\b'
            if re.search(pattern, texto):
                presentes.append(termino)
                contador_terminos[termino] += 1
        
        # üîç DEBUG: Mostrar cu√°ntos t√©rminos aparecen juntos
        if idx < 5:  # Primeros 5 abstracts
            print(f"      Abstract {idx}: {len(presentes)} t√©rminos presentes: {presentes[:5]}...")
        
        # Crear aristas entre t√©rminos que coocurren
        combinaciones = list(itertools.combinations(set(presentes), 2))
        
        for w1, w2 in combinaciones:
            if G.has_edge(w1, w2):
                G[w1][w2]['weight'] += 1
            else:
                G.add_edge(w1, w2, weight=1)
                
            # DEBUG: Guardar las primeras coocurrencias
            if len(coocurrencias_debug) < 10:
                coocurrencias_debug.append((w1, w2))
    
    # üìä MOSTRAR ESTAD√çSTICAS DE DEBUG
    print(f"\nüìä ESTAD√çSTICAS DE CONSTRUCCI√ìN:")
    print(f"   Grafo: {len(G.nodes())} nodos, {len(G.edges())} aristas")
    print(f"\n   üîù Top 10 t√©rminos m√°s frecuentes:")
    for termino, freq in contador_terminos.most_common(10):
        print(f"      {termino}: {freq} apariciones")
    
    print(f"\n   üîó Primeras 10 coocurrencias detectadas:")
    for w1, w2 in coocurrencias_debug[:10]:
        peso = G[w1][w2]['weight'] if G.has_edge(w1, w2) else 0
        print(f"      {w1} <-> {w2}: peso {peso}")
    
    print(f"\n   üìà Distribuci√≥n de grados:")
    grados = dict(G.degree())
    if grados:
        print(f"      M√≠nimo: {min(grados.values())}")
        print(f"      M√°ximo: {max(grados.values())}")
        print(f"      Promedio: {sum(grados.values()) / len(grados):.2f}")
    
    return G

# === Graficar el grafo ===
def _graficar_grafo(G):
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G, seed=42)
    nx.draw_networkx_nodes(G, pos, node_color='lightgreen', node_size=800)
    nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.6)
    nx.draw_networkx_labels(G, pos, font_size=8)
    plt.title("Grafo de Coocurrencia de T√©rminos", fontsize=13)
    plt.axis('off')

    buf = io.BytesIO()
    plt.tight_layout()
    plt.savefig(buf, format='png')
    buf.seek(0)
    img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
    plt.close()
    return img_base64

# === Funci√≥n principal ===
def analizar_grafo_coocurrencia():
    """
    An√°lisis mejorado con filtrado de t√©rminos muy comunes.
    """
    try:
        print("\n" + "="*60)
        print("üî¨ INICIANDO AN√ÅLISIS DE GRAFO DE COOCURRENCIA")
        print("="*60)
        
        # --- 1. Cargar abstracts ---
        abstracts = _cargar_abstracts()
        if not abstracts:
            return {
                "Construcci√≥n autom√°tica del grafo": "‚ö†Ô∏è No se encontraron abstracts.",
                "C√°lculo del grado (t√©rminos m√°s conectados)": [],
                "Detecci√≥n de componentes conexas": {
                    "num_componentes": 0, 
                    "tamano_componentes": []
                },
                "grafico_base64": None
            }

        # --- 2. Definir t√©rminos M√ÅS ESPEC√çFICOS ---
        palabras_clave_base = [
            # Modelos generativos
            "gpt", "bert", "transformer", "gan", "vae", "diffusion",
            
            # T√©cnicas espec√≠ficas
            "fine-tuning", "prompt engineering", "few-shot", "zero-shot",
            "transfer learning", "attention mechanism",
            
            # Aplicaciones
            "text generation", "image generation", "chatbot",
            "natural language processing", "computer vision",
            
            # M√©tricas
            "accuracy", "precision", "recall", "f1-score",
            
            # Conceptos √©ticos
            "bias", "fairness", "explainability", "transparency",
            "privacy", "interpretability"
        ]
        
        terminos_finales = palabras_clave_base.copy()
        
        # --- 3. Obtener t√©rminos de otros requerimientos ---
        if analizador_frecuencias:
            try:
                print("üìä Obteniendo t√©rminos del Requerimiento 3...")
                resultado_req3 = analizador_frecuencias.analizar_frecuencias_completo(
                    abstracts, 
                    palabras_clave_base
                )
                # Solo tomar t√©rminos con frecuencia moderada (no muy comunes ni muy raros)
                terminos_req3 = list(resultado_req3.get("frecuencias_generadas", {}).keys())[:20]
                terminos_finales.extend(terminos_req3)
                print(f"   ‚úÖ Agregados {len(terminos_req3)} t√©rminos del Req 3")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Error obteniendo t√©rminos del Req 3: {e}")

        # --- 4. FILTRAR T√âRMINOS DEMASIADO COMUNES ---
        print("\nüîç Filtrando t√©rminos muy comunes...")
        
        # Contar frecuencia de cada t√©rmino
        frecuencias = Counter()
        for abstract in abstracts:
            texto = abstract.lower()
            for termino in terminos_finales:
                pattern = r'\b' + re.escape(termino.lower()) + r'\b'
                if re.search(pattern, texto):
                    frecuencias[termino] += 1
        
        total_abstracts = len(abstracts)
        
        # Filtrar t√©rminos que aparecen en m√°s del 80% de los abstracts (muy comunes)
        # o en menos del 2% (muy raros)
        terminos_filtrados = []
        for termino, freq in frecuencias.items():
            porcentaje = (freq / total_abstracts) * 100
            if 2 <= porcentaje <= 80:  # Entre 2% y 80%
                terminos_filtrados.append(termino)
            else:
                print(f"   ‚ùå Descartado '{termino}': {porcentaje:.1f}% apariciones")
        
        print(f"\n   ‚úÖ T√©rminos filtrados: {len(terminos_filtrados)} de {len(terminos_finales)}")
        
        if len(terminos_filtrados) < 5:
            print("   ‚ö†Ô∏è Muy pocos t√©rminos despu√©s del filtrado, usando t√©rminos originales")
            terminos_filtrados = terminos_finales[:30]  # Tomar los primeros 30
        
        terminos_finales = list(set(terminos_filtrados))
        print(f"\nüìã Total de t√©rminos √∫nicos despu√©s de filtrado: {len(terminos_finales)}")
        
        if not terminos_finales:
            return {
                "Construcci√≥n autom√°tica del grafo": "‚ö†Ô∏è No hay t√©rminos para construir el grafo.",
                "C√°lculo del grado (t√©rminos m√°s conectados)": [],
                "Detecci√≥n de componentes conexas": {
                    "num_componentes": 0, 
                    "tamano_componentes": []
                },
                "grafico_base64": None
            }

        # --- 5. Construcci√≥n del grafo ---
        print("\nüî® Construyendo grafo de coocurrencia...")
        G = construir_grafo_coocurrencia(abstracts, terminos_finales)
        
        if not G.nodes():
            return {
                "Construcci√≥n autom√°tica del grafo": "‚ö†Ô∏è Grafo vac√≠o (sin coocurrencias detectadas).",
                "C√°lculo del grado (t√©rminos m√°s conectados)": [],
                "Detecci√≥n de componentes conexas": {
                    "num_componentes": 0, 
                    "tamano_componentes": []
                },
                "grafico_base64": None
            }

        # --- 6. FILTRAR ARISTAS CON POCO PESO ---
        print("\nüîç Filtrando aristas con poco peso...")
        aristas_originales = len(G.edges())
        
        # Calcular peso m√≠nimo (por ejemplo, 5% del m√°ximo)
        pesos = [data['weight'] for _, _, data in G.edges(data=True)]
        if pesos:
            peso_max = max(pesos)
            peso_min_threshold = max(2, peso_max * 0.05)  # Al menos 2 o 5% del m√°ximo
            
            # Crear nuevo grafo solo con aristas significativas
            G_filtrado = nx.Graph()
            G_filtrado.add_nodes_from(G.nodes(data=True))
            
            for u, v, data in G.edges(data=True):
                if data['weight'] >= peso_min_threshold:
                    G_filtrado.add_edge(u, v, weight=data['weight'])
            
            # Eliminar nodos aislados
            nodos_aislados = list(nx.isolates(G_filtrado))
            G_filtrado.remove_nodes_from(nodos_aislados)
            
            print(f"   Aristas antes: {aristas_originales}")
            print(f"   Aristas despu√©s: {len(G_filtrado.edges())}")
            print(f"   Nodos eliminados (aislados): {len(nodos_aislados)}")
            
            G = G_filtrado

        # --- 7. Generar gr√°fico ---
        print("\nüé® Generando visualizaci√≥n...")
        grafico_base64 = _graficar_grafo(G)

        # --- 8. C√°lculo de grado ---
        print("\nüìä Calculando estad√≠sticas del grafo...")
        grados = dict(G.degree())
        top_grado = sorted(grados.items(), key=lambda x: x[1], reverse=True)[:15]

        # --- 9. Componentes conexas ---
        componentes = list(nx.connected_components(G))
        tamanos_componentes = sorted([len(c) for c in componentes], reverse=True)

        # --- 10. Estad√≠sticas adicionales ---
        densidad = nx.density(G)
        grado_medio = sum(dict(G.degree()).values()) / len(G.nodes()) if G.nodes() else 0

        print("\n" + "="*60)
        print("‚úÖ AN√ÅLISIS COMPLETADO")
        print(f"   üìç Nodos: {len(G.nodes())}")
        print(f"   üîó Aristas: {len(G.edges())}")
        print(f"   üìä Densidad: {densidad:.4f}")
        print(f"   üìà Grado medio: {grado_medio:.2f}")
        print(f"   üî¥ Componentes: {len(componentes)}")
        print("="*60 + "\n")

        return {
            "Construcci√≥n autom√°tica del grafo": f"‚úÖ Grafo creado con {len(G.nodes())} t√©rminos y {len(G.edges())} relaciones.",
            "C√°lculo del grado (t√©rminos m√°s conectados)": top_grado,
            "Detecci√≥n de componentes conexas": {
                "num_componentes": len(componentes),
                "tamano_componentes": tamanos_componentes
            },
            "estadisticas_adicionales": {
                "densidad": round(densidad, 4),
                "grado_medio": round(grado_medio, 2),
                "nodos": len(G.nodes()),
                "aristas": len(G.edges())
            },
            "grafico_base64": grafico_base64
        }

    except FileNotFoundError as e:
        print(f"‚ùå Archivo no encontrado: {e}")
        return {"error": f"Archivo no encontrado: {str(e)}"}
    except Exception as e:
        print(f"‚ùå Error interno en analizar_grafo_coocurrencia: {e}")
        import traceback
        traceback.print_exc()
        return {"error": f"Error interno: {str(e)}"}